# MobileNetV2
### 一.简介
MobileNetV2相比于V1最大的特点是引入了一个新模块——具有倒残差的线性瓶颈，操作过程如下：
* 输入用低维压缩表示，先扩展到高维
* 用轻量级深度卷积对其进行滤波
* 利用线性卷积将特征投影回低维表示
### 二.预备知识
1. 深度可分离卷积：
可分离为深度卷积和点卷积，深度卷积对每个输入通道进行滤波，点卷积对深度卷积输出进行合成
2. 线性瓶颈
* 具有n层网络的神经网络每一层都有激活张量，表示为：h * w * d,可以将其视为具有d维度的h * w的像素
* 所有的激活张量构成兴趣流，激活张量的信息保存在兴趣流之中，被长时间认为可以被嵌入到低维子空间中
* 直觉认为可以通过降低层的维度来降低操作维度，但在非线性坐标变换操作如ReLu之中这种直觉会失效，因为ReLu会被线性转换所限制
* 深度网络对容量非零的输出域只有线性分类的能力
* 证明兴趣流应该嵌在高维度激活空间的低纬度子空间的两个性质：
（1）如果兴趣流在ReLu变换之后还具有非零容量，则它应该对应一个线性转换
（2）ReLu操作在输入低维子空间中可以保存全部的信息
* 线性层在神经网络中很重要，因为他可以防止非线性操作破坏太多信息
* 膨胀比：输入瓶颈大小/瓶颈内部大小
3. 倒残差
* 瓶颈块和倒残块组成类似：输入+瓶颈+膨胀
* 我们使用连接瓶颈的快捷方式是类似于倒残块的
* 对于一个h * w的块，膨胀比为t,内核大小为k,输入通道为d1,输出通道是d2,则这个块总的加乘量为：h * w * d1 *t * (d1 + k^2 + d2)
4. 信息流动的解释
* 在瓶颈层输入输出域（神经网络每层的容量）和层转换（一种操作）之间有一个自然的分离，是一个将输入转为输出的非线性函数，
### 三.MobileNetV2的架构
* MobileNetV2第一层是具有32个过滤器的全卷积层；接下来是19个具有倒残差的线性瓶颈模块；我们的非线性操作是ReLu6(低分辨率计算的鲁棒性)；用3 * 3内核；并且在训练过程中使用归一化
* 在整个网络中用恒定的膨胀比，并且在膨胀比为5-10的时候没有明显差别；小的网络更适合小的膨胀比，大的网络更适合大的膨胀比
* 对于两个超参数（宽度和分辨率乘法器),仍然可以通过调整他们来调整模型得到不同的计算量和模型大小
* 当宽度乘法器小于1的时候，我们将他应用于除了最后一层卷积层的所有层
### 四.实现的注意事项
1. 内存高效推理：
* 在具有倒残差瓶颈块的层中，在高效推理实现中构建了一个有向无环的超图G，超图G是由表操作的边和中间计算张量的节点所组成，它可以计算排序所有可能的计算量并取得最小值
* 内存的大小是所有操作中输入输出合并的最大值，如果把倒残差瓶颈模块看成一个单个操作，则内存大小会被瓶颈张量所控制
2. 倒残差瓶颈块：倒残差瓶颈操作可以分解为三个操作：A线性转换，N每个通道的非线性转换，B对输出域的线性转换；其计算量与线性和非线性转换的维度和内核大小有关
### 五.实验：
1. 数据集分类：MobileNetV2相比于MobileNetV1和ShuffleNet网络等参数更少，模型更小，cpu运行速度更快
2. 目标检测：
* SSD是单个射击检测器，将所有SSD中的正则卷积转换为深度可分离卷积则称为SSDLite，它会减少更多的参数和计算量
3. 语义分割：
为了构建一个模型，我们用了三种设计变体：
* 不同的特征提取器
* 简化的DeepLabV3头部
* 不同的推理策略
我们得到以下结论：
* 推理策略不适合在设备上用
* 步幅16比8好
* MobileNetV1是一个强大的特征提取器
* 将DeepLabv3头部构建在倒数第二层比在倒数第一层更好
* DeepLabv3头部计算量很高
4. 消融实验
* 快捷方式连接瓶颈块比连接扩展更好
* 尽管线性瓶颈更容易被激活，但是他的性能更强大 

































